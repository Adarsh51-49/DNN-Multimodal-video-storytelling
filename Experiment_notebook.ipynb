{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4b2e79",
   "metadata": {},
   "source": [
    "# Improving Long-Range Multimodal Video Storytelling Using Transformer-Based Architectures\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project is about generating stories from videos using both images and text. A common method is to use CNN models to extract image features and LSTM models to understand the sequence over time.\n",
    "\n",
    "LSTM-based models work well for short videos but face problems when the video is long. Important information from earlier frames can be lost, which results in incomplete or unclear captions.\n",
    "\n",
    "To solve this problem, this project uses a Transformer-based model. Transformers use attention to look at all frames together, which helps the model understand long videos and produce more meaningful stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88819cfb",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "## Dataset\n",
    "\n",
    "This project uses the StoryReasoning Dataset. The dataset contains sequences of images along with their corresponding text descriptions, which together form a visual story.\n",
    "\n",
    "Each sequence is treated as a video, and the task is to predict the next part of the story based on previous images and text. To avoid data leakage, the data is split at the video level.\n",
    "\n",
    "- Training data: 80%\n",
    "- Validation data: 10%\n",
    "- Test data: 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28567aa3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
